# ADR-001: Kubernetes Deployment Strategy Using Bitnami Helm Charts

> **Scope**: K8s infrastructure deployment strategy across Kafka, PostgreSQL, and application services.

- **Status:** Accepted
- **Date:** 2026-01-31
- **Feature:** kafka-k8s-setup, postgres-k8s-setup, nextjs-k8s-deploy
- **Context:** LearnFlow platform requires Kubernetes-native deployment for all infrastructure components (Kafka, PostgreSQL) and application services. Need a consistent, maintainable approach for deploying and managing these services across development (Minikube) and production (AKS/GKE/EKS) environments.

<!-- Significance checklist (ALL must be true to justify this ADR)
     1) Impact: Long-term consequence for architecture/platform/security? ✅ YES - K8s is target production environment
     2) Alternatives: Multiple viable options considered with tradeoffs? ✅ YES - Custom manifests, operators, alternative Helm charts
     3) Scope: Cross-cutting concern (not an isolated detail)? ✅ YES - Affects all infrastructure deployments
     If any are false, prefer capturing as a PHR note instead of an ADR. -->

## Decision

**Use Bitnami Helm Charts as the standard deployment method for all Kubernetes infrastructure.**

- **Kafka**: Bitnami Kafka Helm chart (for event-driven messaging backbone)
- **PostgreSQL**: Bitnami PostgreSQL Helm chart (for relational data persistence)
- **Application Services**: K8s manifests generated by skills (for microservices and frontend)
- **Helm Version**: 3.x required
- **Cluster Support**: Minikube (dev), AKS, GKE, EKS (production)

**Rationale**: Bitnami charts are production-grade, actively maintained, provide sensible defaults, support custom values via YAML, and include built-in health checks and PVC management.

## Consequences

### Positive

- **Rapid Deployment**: Kafka and PostgreSQL deploy in <5 minutes with single `helm install` command
- **Production-Ready**: Bitnami charts include security best practices, resource limits, and high availability configurations
- **Upgradability**: `helm upgrade` enables zero-downtime updates with rollback capability
- **Consistency**: Same deployment method across dev (Minikube) and prod (cloud providers)
- **Maintenance**: Bitnami actively updates charts for security patches and new features
- **Community Support**: Large user base, extensive documentation, and troubleshooting resources
- **Helm Ecosystem**: Integration with Helm plugins, diff tools, and CI/CD pipelines

### Negative

- **Abstraction Layer**: Helm adds abstraction over raw K8s manifests (need to understand Helm templating)
- **Chart Updates**: Bitnami chart updates may introduce breaking changes (requires testing before upgrading)
- **Resource Overhead**: Helm requires Tiller (v2) or Helm 3 binary installation (minor operational overhead)
- **Custom Values Complexity**: Complex custom configurations require deep understanding of chart values.yaml
- **Vendor Opinion**: Bitnami's opinions on defaults may not match all use cases (requires customization)

## Alternatives Considered

### Alternative A: Custom Kubernetes Manifests

- **Approach**: Write raw K8s YAML files for Deployment, Service, ConfigMap, PVC, etc.
- **Why Rejected**:
  - 10x more YAML to maintain (every resource defined explicitly)
  - No built-in upgrade/rollback mechanism
  - No community-tested defaults
  - Higher risk of misconfiguration
  - Longer deployment times (manual kubectl apply for each resource)

### Alternative B: Kubernetes Operators

- **Approach**: Use Strimzi (Kafka Operator) and Zalando (PostgreSQL Operator)
- **Why Rejected**:
  - Additional operational complexity (operator lifecycle management)
  - Overkill for current scale (single cluster, <100 services)
  - Steeper learning curve for team
  - Vendor lock-in to operator-specific CRDs
  - Benefits (auto-scaling, self-healing) not needed for MVP

### Alternative C: Managed Cloud Services (MSK, Cloud SQL, etc.)

- **Approach**: Use AWS MSK/Cloud SQL, GCP Pub/Sub/Cloud SQL, Azure Event Hubs/SQL
- **Why Rejected**:
  - Vendor lock-in to specific cloud provider
  - Higher costs at development scale
  - Harder to run locally (Minikube) for development
  - Network complexity between cloud provider and K8s cluster
  - Not aligned with "Kubernetes-Native" constitution principle

## References

- Kafka Spec: [specs/2-kafka-k8s-setup/spec.md](../specs/2-kafka-k8s-setup/spec.md)
- Kafka Plan: [specs/2-kafka-k8s-setup/plan.md](../specs/2-kafka-k8s-setup/plan.md)
- PostgreSQL Spec: [specs/3-postgres-k8s-setup/spec.md](../specs/3-postgres-k8s-setup/spec.md)
- PostgreSQL Plan: [specs/3-postgres-k8s-setup/plan.md](../specs/3-postgres-k8s-setup/plan.md)
- Constitution Principle VII: [Kubernetes-Native Deployment](../.specify/memory/constitution.md#vii-kubernetes-native-deployment)
- Bitnami Kafka Chart: https://github.com/bitnami/charts/tree/main/bitnami/kafka
- Bitnami PostgreSQL Chart: https://github.com/bitnami/charts/tree/main/bitnami/postgresql
